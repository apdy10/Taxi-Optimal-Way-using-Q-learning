{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Taxi Optimal Way using Q learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPj0CfKMuO80mIYkhA+IbKQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apdy10/Taxi-Optimal-Way-using-Q-learning/blob/main/Taxi_Optimal_Way_using_Q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4yX_68yQiEb4"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"Taxi-v3\")\n",
        "env"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28nd_AFqiRFF",
        "outputId": "b406910e-a2c5-43e0-898c-04bb99b5bad3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TimeLimit<TaxiEnv<Taxi-v3>>>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.reset()\n",
        "state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUwHwQAdiphF",
        "outputId": "98e9081c-8b45-4255-fbe5-d7e5c48982a6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "314"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.observation_space.n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvoEEH_SjXOR",
        "outputId": "3bd5a6a9-d231-430a-e39d-4e371a0a1205"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BNGWGFCjb8r",
        "outputId": "6d4dfa8a-2e12-42e8-979b-06c13b909162"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m| : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abTftT88PCz5",
        "outputId": "8d7a47dc-1ebf-4492-8bd2-a473a86fc887"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "314"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeS0V70TPHMx",
        "outputId": "4390cf75-3c55-40b2-aadc-8a433972ce90"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m| : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Possible Actions** - \n",
        "Down(0), Up(1), Right(2), Left(3), Pickup(4) and drop-off(5)\n"
      ],
      "metadata": {
        "id": "3hg_E9YtPbKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_states = env.observation_space.n\n",
        "n_actions = env.action_space.n"
      ],
      "metadata": {
        "id": "hwc72cuxPtlQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_actions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCryzMq3QCWO",
        "outputId": "f6a0e6c7-2aa1-4227-b878-88ec55b6c657"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.env.s = 150"
      ],
      "metadata": {
        "id": "Kf7OLrJCQGSb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.render()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miHObO58QKMi",
        "outputId": "ea73f428-bd96-4f2b-f9b6-a49c89c6801d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : |\u001b[43m \u001b[0m: : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[34;1mY\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.step(1)     #prob : 1 means that the passenger is in the cab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-HCM6FkQVlp",
        "outputId": "eb365b2f-c1c1-4aa8-b4dd-11e7b6e6a421"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50, -1, False, {'prob': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Behaving Completely Random do"
      ],
      "metadata": {
        "id": "6AtSXvz6Qwe3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Presetting the Variables**"
      ],
      "metadata": {
        "id": "VNVP7XgTQ6JJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.reset()\n",
        "counter = 0   #counting how many directions that particular taxy has made\n",
        "g = 0         #goal state whether the agent is successfull in dropping the agent at correct location\n",
        "reward = None   #if it's successfull in picking up the location where the passenger is that will be true and when it drop the passenger at particular location it will be true but in case of driving the passenger in between the paths this will be false, false means the cab and passenger are in state of mmotion\n",
        "\n"
      ],
      "metadata": {
        "id": "Rvp8Ak2DQ5GY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while reward!=20:\n",
        "  state, reward, done, info = env.step(env.action_space.sample())  #step perform the action at each step so that at each step it will move from one state to another state, at every state you have a reward whenever you reach from st to st+1 you get a reward it can be +ve or it can be -ve \n",
        "  #done can be true in 2 possible states when it is at pickup location or when it is picking up the passenger so there done value is true or when it has dropped off at correct location indicated by the passenger else every other times it is false, the info is current state of the taxi\n",
        "  counter = counter+1  \n",
        "  g = g+reward         #At each state you get particular reward regardless of positive or negative reward we have to train whenever we collect the reward we reach the goal state. At goal state we have to maximise the reward so I have to accumulate the reward "
      ],
      "metadata": {
        "id": "qnqjoymtShO2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Solved in {} steps with a total reward of {}\". format(counter,g))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ibr6zTBW6lZ",
        "outputId": "b727f776-b004-469f-9a24-51e5627e0ad8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solved in 1619 steps with a total reward of -6251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agent in case of randomised pattern so our goal is to decrease the steps and maximize the rewards"
      ],
      "metadata": {
        "id": "WZiiLd7NZkGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How Q value changes after each episode"
      ],
      "metadata": {
        "id": "eDW_eolKbRnX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q function - extension of bellman equation \n",
        "\n",
        "Q table - consists of all the possible states with all the possible actions states as rows(checked earlier upper part) and actions as columns"
      ],
      "metadata": {
        "id": "yhjwgneOb0Pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q = np.zeros([n_states, n_actions])"
      ],
      "metadata": {
        "id": "icMr8nsqbcYJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wdjIr9_bsi_",
        "outputId": "13d9f6e4-394e-4764-aee8-74c3ea4552f3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = 1     #for how many episodes we want this agent to train as of now it's 1 let's see how it can be improved over time period\n",
        "G = 0\n",
        "alpha = 0.618    #learning rate completely random"
      ],
      "metadata": {
        "id": "V_QjDbtscZx9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(1, episodes+1):\n",
        "  done = False                  #done is final state when passenger drop at final destination then it is true\n",
        "  G, reward = 0,0              \n",
        "  state = env.reset()        #whenever we do new iteration we reset the environment as sometime the old iteration is there\n",
        "  firstState = state         #whatever is current state is my first state\n",
        "  print(\"Initial State = {}\".format(state))\n",
        "  while reward!=20:               #if it is 20 it reach the goal state\n",
        "    action = np.argmax(Q[state])   #action is dependent upon the state so dependent upon state it maximises the action which we have to perform  \n",
        "    state2, reward, done, info = env.step(action)    #the current action that is from the first state it will take the second state the it will get the reward then it will check whether it has reached the destination or not if it's the destination done is changed to true, info has current status of that agent \n",
        "    Q[state, action] += alpha*(reward + np.max(Q[state2]) - Q[state,action])\n",
        "    G = G+reward\n",
        "    state = state2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73n922EYdobZ",
        "outputId": "e2c3d868-39bb-4ef9-baa5-7642b2f52cae"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial State = 249\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finalState = state"
      ],
      "metadata": {
        "id": "ygOl6RdYe87Y"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finalState"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsDbcLF_e_1s",
        "outputId": "bfe9d8a7-1d92-40d7-8507-2e0606a3c694"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "85"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At first step:"
      ],
      "metadata": {
        "id": "630gbgOEfNOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "firstState"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4BWFkt_fQYQ",
        "outputId": "078d59e3-2db4-42b7-b6a0-5de847e6ed75"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "249"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At final Step:"
      ],
      "metadata": {
        "id": "squiDvPefTMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finalState"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdOmYQy_fSa_",
        "outputId": "5cb78bf8-0d71-48ee-a921-3bc0b50a493e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "85"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U53twHlgfbeJ",
        "outputId": "efe965e1-8593-4910-d03d-267978bdf787"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ],\n",
              "       [-1.88844915, -1.854     , -2.090076  , -1.854     , -1.96546376,\n",
              "        -6.18      ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ],\n",
              "       ...,\n",
              "       [-1.236     , -1.236     , -0.618     , -0.618     , -6.18      ,\n",
              "        -6.18      ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now run over multiple episodes so that we can converge on a optimal policy"
      ],
      "metadata": {
        "id": "R3X2470PfhD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = 4000\n",
        "rewardTracker = []"
      ],
      "metadata": {
        "id": "Lzhg3M-ffqdc"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G = 0\n",
        "alpha = 0.618"
      ],
      "metadata": {
        "id": "OBe7mTGOfxyk"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(1, episodes+1):\n",
        "  done = False\n",
        "  G, reward = 0,0\n",
        "  state = env.reset()\n",
        "  while done!= True:\n",
        "    action = np.argmax(Q[state])\n",
        "    state2, reward, done, info = env.step(action)\n",
        "    Q[state, action] += alpha*(reward + np.max(Q[state2]) - Q[state,action])\n",
        "    G = G+reward\n",
        "    state = state2\n",
        "\n",
        "  if episode%100 == 0:             #To print all the 4000 episode with their rewards will not make sense so only multiples of 100 to be used\n",
        "    print('Episode {} Total Reward: {}'.format(episode,G))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIPKQnELf0p1",
        "outputId": "a96b521a-ef6f-4bb3-d88f-6fbee92f2601"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100 Total Reward: -131\n",
            "Episode 200 Total Reward: 13\n",
            "Episode 300 Total Reward: -27\n",
            "Episode 400 Total Reward: 8\n",
            "Episode 500 Total Reward: 6\n",
            "Episode 600 Total Reward: 7\n",
            "Episode 700 Total Reward: 4\n",
            "Episode 800 Total Reward: 6\n",
            "Episode 900 Total Reward: 10\n",
            "Episode 1000 Total Reward: 10\n",
            "Episode 1100 Total Reward: 8\n",
            "Episode 1200 Total Reward: 4\n",
            "Episode 1300 Total Reward: 4\n",
            "Episode 1400 Total Reward: 6\n",
            "Episode 1500 Total Reward: 9\n",
            "Episode 1600 Total Reward: 5\n",
            "Episode 1700 Total Reward: 6\n",
            "Episode 1800 Total Reward: 6\n",
            "Episode 1900 Total Reward: 10\n",
            "Episode 2000 Total Reward: 6\n",
            "Episode 2100 Total Reward: 8\n",
            "Episode 2200 Total Reward: 9\n",
            "Episode 2300 Total Reward: 7\n",
            "Episode 2400 Total Reward: 8\n",
            "Episode 2500 Total Reward: 14\n",
            "Episode 2600 Total Reward: 11\n",
            "Episode 2700 Total Reward: 5\n",
            "Episode 2800 Total Reward: 6\n",
            "Episode 2900 Total Reward: 9\n",
            "Episode 3000 Total Reward: 8\n",
            "Episode 3100 Total Reward: 3\n",
            "Episode 3200 Total Reward: 8\n",
            "Episode 3300 Total Reward: 8\n",
            "Episode 3400 Total Reward: 5\n",
            "Episode 3500 Total Reward: 9\n",
            "Episode 3600 Total Reward: 6\n",
            "Episode 3700 Total Reward: 9\n",
            "Episode 3800 Total Reward: 9\n",
            "Episode 3900 Total Reward: 4\n",
            "Episode 4000 Total Reward: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have the optimal Q values so we don't have to train the agent anymore"
      ],
      "metadata": {
        "id": "r3wU2nT7hDaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.reset()\n",
        "\n",
        "done = None"
      ],
      "metadata": {
        "id": "REssqcIihK4R"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while done!= True:\n",
        "  action = np.argmax(Q[state])\n",
        "  state, reward, done, info = env.step(action)\n",
        "  env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl1i8OzShP2D",
        "outputId": "fdf001ec-e88e-49e6-8b9e-37319f97b064"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m| : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[43m \u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[43m \u001b[0m: : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[42mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[42m_\u001b[0m: |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[42m_\u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[42m_\u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[42m_\u001b[0m: : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[42m_\u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "|\u001b[42m_\u001b[0m: | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n"
          ]
        }
      ]
    }
  ]
}